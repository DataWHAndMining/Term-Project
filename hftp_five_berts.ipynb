{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvDUnuefyT8l"
      },
      "outputs": [],
      "source": [
        "# Install the necessary library\n",
        "!pip install transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the dataset (upload your dataset to Colab before running this code)\n",
        "file_path = '/content/Twitter_Data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename the columns for clarity\n",
        "df.columns = ['text', 'sentiment']\n",
        "\n",
        "# Filter out rows with missing values\n",
        "df_clean = df.dropna()\n",
        "\n",
        "# Select a sample of 10,000 rows\n",
        "df_sample = df_clean.sample(n=10000, random_state=1)\n",
        "\n",
        "# List of Hugging Face transformer models to test\n",
        "models = [\n",
        "    'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "    'nlptown/bert-base-multilingual-uncased-sentiment',\n",
        "    'cardiffnlp/twitter-roberta-base-sentiment',\n",
        "    'siebert/sentiment-roberta-large-english',\n",
        "    'finiteautomata/bertweet-base-sentiment-analysis'\n",
        "]\n",
        "\n",
        "# Function to map the pipeline's output to our sentiment labels\n",
        "def map_sentiment(label):\n",
        "    if label in ['negative', 'NEGATIVE']:\n",
        "        return -1\n",
        "    elif label in ['positive', 'POSITIVE']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Dictionary to store performance metrics for each model\n",
        "performance_metrics = {}\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name in models:\n",
        "    # Load pre-trained sentiment analysis pipeline for the current model\n",
        "    sentiment_pipeline = pipeline('sentiment-analysis', model=model_name, device=0)\n",
        "\n",
        "    # Perform sentiment analysis and add predictions to the dataset\n",
        "    df_sample['hf_predicted_sentiment'] = df_sample['text'].apply(\n",
        "        lambda x: map_sentiment(sentiment_pipeline(x)[0]['label'])\n",
        "    )\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy = accuracy_score(df_sample['sentiment'], df_sample['hf_predicted_sentiment'])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for each class\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(df_sample['sentiment'], df_sample['hf_predicted_sentiment'], average='weighted')\n",
        "\n",
        "    # Compile the metrics into a dictionary\n",
        "    performance_metrics[model_name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# Display the performance metrics for each model\n",
        "for model_name, metrics in performance_metrics.items():\n",
        "    print(f\"Performance Metrics for {model_name}:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
        "    print(f\"Precision: {metrics['precision']}\")\n",
        "    print(f\"Recall: {metrics['recall']}\")\n",
        "    print(f\"F1-score: {metrics['f1_score']}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Convert performance metrics to a DataFrame for better visualization\n",
        "metrics_df = pd.DataFrame(performance_metrics).transpose()\n",
        "\n",
        "# Plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Comparison of Hugging Face Transformer Models', fontsize=16)\n",
        "\n",
        "# Accuracy Plot\n",
        "axes[0, 0].bar(metrics_df.index, metrics_df['accuracy'], color='skyblue')\n",
        "axes[0, 0].set_title('Accuracy')\n",
        "axes[0, 0].set_xticklabels(metrics_df.index, rotation=45, ha='right')\n",
        "\n",
        "# Precision Plot\n",
        "axes[0, 1].bar(metrics_df.index, metrics_df['precision'], color='lightgreen')\n",
        "axes[0, 1].set_title('Precision')\n",
        "axes[0, 1].set_xticklabels(metrics_df.index, rotation=45, ha='right')\n",
        "\n",
        "# Recall Plot\n",
        "axes[1, 0].bar(metrics_df.index, metrics_df['recall'], color='lightcoral')\n",
        "axes[1, 0].set_title('Recall')\n",
        "axes[1, 0].set_xticklabels(metrics_df.index, rotation=45, ha='right')\n",
        "\n",
        "# F1 Score Plot\n",
        "axes[1, 1].bar(metrics_df.index, metrics_df['f1_score'], color='lightsalmon')\n",
        "axes[1, 1].set_title('F1 Score')\n",
        "axes[1, 1].set_xticklabels(metrics_df.index, rotation=45, ha='right')\n",
        "\n",
        "# Adjust layout and show plots\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    }
  ]
}