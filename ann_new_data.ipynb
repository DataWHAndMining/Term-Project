{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get twitter data files\n",
    "zip_path = './new_data'  \n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Create a directory to extract files to if it doesn't exist\n",
    "os.makedirs(current_directory, exist_ok=True)\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_path = os.path.join(current_directory, './new_data/Twitter_Data.csv')\n",
    "\n",
    "# Read the CSV files into pandas DataFrames\n",
    "testing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the datasets\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['clean_text'] = testing_data['clean_text'].astype(str)\n",
    "\n",
    "testing_data.dropna(subset=['clean_text'], inplace=True)\n",
    "\n",
    "# Apply the tokenizer\n",
    "sequences = tokenizer.texts_to_sequences(testing_data['clean_text'])\n",
    "data_features = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining non-string entries\n",
    "if any(type(item) != str for item in testing_data['clean_text']):\n",
    "    print(\"Non-string data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original predictions: 162973\n",
      "Data rows after preprocessing: 162980\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original predictions: {len(predicted_labels)}\")\n",
    "print(f\"Data rows after preprocessing: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'category' is NaN\n",
    "testing_data.dropna(subset=['category'], inplace=True)\n",
    "\n",
    "label_map = {-1.0: 'negative', 0.0: 'neutral', 1.0: 'positive'}\n",
    "testing_data['category'] = testing_data['category'].map(label_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predicted_labels) != testing_data.shape[0]:\n",
    "    valid_indices = testing_data.index  # Indices that remained after preprocessing\n",
    "    predicted_labels = [predicted_labels[i] for i in valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['negative', 'neutral', 'positive']) \n",
    "encoded_labels = encoder.transform(testing_data['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5094/5094 [==============================] - 8s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = load_model('./nnmodel.h5')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(data_features)\n",
    "predicted_labels = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predicted_labels) != testing_data.shape[0]:\n",
    "    valid_indices = testing_data.index  # Indices that remained after preprocessing\n",
    "    predicted_labels = [predicted_labels[i] for i in valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your model outputs encoded labels, decode them\n",
    "predicted_labels = encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:, 44.33249679394746%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermondy/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/cartermondy/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00     35510\n",
      "     neutral       0.00      0.00      0.00     55213\n",
      "    positive       0.44      1.00      0.61     72250\n",
      "\n",
      "    accuracy                           0.44    162973\n",
      "   macro avg       0.15      0.33      0.20    162973\n",
      "weighted avg       0.20      0.44      0.27    162973\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermondy/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(testing_data['category'], predicted_labels)\n",
    "print(f'Accuracy:, {accuracy * 100}%')\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(testing_data['category'], predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
