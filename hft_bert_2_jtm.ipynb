{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kNxh1LMckC9B",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "JM-7ohvqnhRf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w1sCJWzW7f8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "pS2O7gSK7qSU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import huggingface_hub\n",
        "\n",
        "df = pd.read_csv('/content/Twitter_Data.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "training_data, validation_data = train_test_split(df, test_size=0.2, random_state=71)\n",
        "\n",
        "columns = [\"content\", \"sentiment\"]\n",
        "training_data.columns = columns\n",
        "validation_data.columns = columns\n",
        "\n",
        "# Encode sentiment labels\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Iterate over the 'sentiment' column using .items()\n",
        "all_sentiments = [label for _, label in training_data['sentiment'].items()] + [label for _, label in validation_data['sentiment'].items()]\n",
        "le.fit(all_sentiments)\n",
        "training_labels = le.transform([label for _, label in training_data['sentiment'].items()])\n",
        "validation_labels = le.transform([label for _, label in validation_data['sentiment'].items()])\n",
        "\n",
        "# Load pre-trained model and tokenizer with increased timeout\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, timeout=180)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
        "\n",
        "# Tokenize data\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples, truncation=True, padding=True)\n",
        "\n",
        "training_texts = training_data['content'].tolist()\n",
        "validation_texts = validation_data['content'].tolist()\n",
        "tokenized_training_data = preprocess_function(training_texts)\n",
        "tokenized_validation_data = preprocess_function(validation_texts)\n",
        "\n",
        "# Create PyTorch datasets\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(tokenized_training_data, training_labels)\n",
        "eval_dataset = SentimentDataset(tokenized_validation_data, validation_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Create Trainer and train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./saved_model_2\")\n",
        "tokenizer.save_pretrained(\"./saved_model_2\")"
      ],
      "metadata": {
        "id": "9BSBzuicxA_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Wrap tokenized data in a Dataset object\n",
        "predict_dataset = SentimentDataset(tokenized_validation_data, validation_labels)\n",
        "\n",
        "predictions = trainer.predict(predict_dataset)\n",
        "predicted_labels = predictions.predictions.argmax(axis=1)\n",
        "\n",
        "accuracy = accuracy_score(validation_labels, predicted_labels) # Use validation_labels instead of validation_data['sentiment']\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(validation_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "RIBE-WwhwGjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload model and test on different dataset"
      ],
      "metadata": {
        "id": "T5ozl_WDpYlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Specify the directory where the model was saved\n",
        "model_directory = \"./saved_model_2\"\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
        "\n",
        "# Create a pipeline for sequence classification\n",
        "classifier = pipeline(\"text-classification\", model=loaded_model, tokenizer=loaded_tokenizer, return_all_scores=False)\n",
        "\n",
        "df = pd.read_csv('/content/Twitter_Data.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "test_text = df['content'].tolist()\n",
        "test_labels = df['sentiment'].tolist()\n",
        "\n",
        "predictions = classifier(test_text)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "accuracy = sum(1 for pred, label in zip(predictions, test_labels) if pred == label) / len(test_labels)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "b3f2sUR6pexy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}