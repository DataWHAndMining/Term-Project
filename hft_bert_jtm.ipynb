{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.28.1"
      ],
      "metadata": {
        "id": "kNxh1LMckC9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.12.0"
      ],
      "metadata": {
        "id": "JM-7ohvqnhRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import csv\n",
        "\n",
        "# Load data from CSV files (no headers)\n",
        "def load_data(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        data = [(row[3], row[2]) for row in reader]\n",
        "    return data\n",
        "\n",
        "training_data = load_data('/content/twitter_training.csv')\n",
        "validation_data = load_data('/content/twitter_validation.csv')\n",
        "\n",
        "# Encode sentiment labels\n",
        "le = LabelEncoder()\n",
        "all_sentiments = [label for _, label in training_data] + [label for _, label in validation_data]\n",
        "le.fit(all_sentiments)\n",
        "training_labels = le.transform([label for _, label in training_data])\n",
        "validation_labels = le.transform([label for _, label in validation_data])\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
        "\n",
        "# Tokenize data\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "training_texts = [text for text, _ in training_data]\n",
        "validation_texts = [text for text, _ in validation_data]\n",
        "tokenized_training_data = preprocess_function(training_texts)\n",
        "tokenized_validation_data = preprocess_function(validation_texts)\n",
        "\n",
        "# Create PyTorch datasets\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(tokenized_training_data, training_labels)\n",
        "eval_dataset = SentimentDataset(tokenized_validation_data, validation_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Create Trainer and train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "9pheiWlbrUV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}